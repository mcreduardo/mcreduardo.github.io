<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="icon" href="https://img.icons8.com/ios/50/000000/developer.png">


    <meta name="author" content="Eduardo Rocha">
    <meta name="description" content="Eduardo Rocha&#39;s personal website">
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="iMaterialist (Fashion) 2019 at FGVC6 - Mask_RCNN"/>
<meta name="twitter:description" content="GitHub repository
Challenge iMaterialist (Fashion) 2019 at FGVC6: Fine-grained segmentation task for fashion and apparel
Task &ldquo;We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.&rdquo;
Approach Mask R-CNN (paper) was chosen because it is currently among the best methods for this task (top 10 in Instance Segmentation on COCO), and it is fairly simple to be implemented."/>

    <meta property="og:title" content="iMaterialist (Fashion) 2019 at FGVC6 - Mask_RCNN" />
<meta property="og:description" content="GitHub repository
Challenge iMaterialist (Fashion) 2019 at FGVC6: Fine-grained segmentation task for fashion and apparel
Task &ldquo;We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.&rdquo;
Approach Mask R-CNN (paper) was chosen because it is currently among the best methods for this task (top 10 in Instance Segmentation on COCO), and it is fairly simple to be implemented." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mcreduardo.github.io/projects/fashion/" />



    <base href="https://mcreduardo.github.io/projects/fashion/">
    <title>
  iMaterialist (Fashion) 2019 at FGVC6 - Mask_RCNN · Eduardo Rocha
</title>

    <link rel="canonical" href="https://mcreduardo.github.io/projects/fashion/">

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" integrity="sha256-oSrCnRYXvHG31SBifqP2PM1uje7SJUyX0nTwO2RJV54=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://mcreduardo.github.io/css/coder.min.c1be03146f97da86bebba55626e5a49a57e62e8332c983f1c234d9c1261a59ee.css" integrity="sha256-wb4DFG&#43;X2oa&#43;u6VWJuWkmlfmLoMyyYPxwjTZwSYaWe4=" crossorigin="anonymous" media="screen" />
    

    

    

    
      <link rel="stylesheet" href="https://mcreduardo.github.io/css/custom.css">
    

    <link rel="icon" type="image/png" href="https://mcreduardo.github.io/img/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://mcreduardo.github.io/img/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.54.0" />
  </head>

  <body class=" ">
    <main class="wrapper">
      
<nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://mcreduardo.github.io/">
      Eduardo Rocha
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://mcreduardo.github.io/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://mcreduardo.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://mcreduardo.github.io/resume.pdf">Resume</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://mcreduardo.github.io/contact/">Contact</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>iMaterialist (Fashion) 2019 at FGVC6 - Mask_RCNN</h1>
    </header>

    

<p><a href="https://github.com/mcreduardo/fashionChallenge-SemSegmentation">GitHub repository</a></p>

<h1 id="challenge">Challenge</h1>

<p><a href="https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6">iMaterialist (Fashion) 2019 at FGVC6</a>:
Fine-grained segmentation task for fashion and apparel</p>

<h3 id="task">Task</h3>

<p>&ldquo;We present a new clothing dataset with the goal of introducing a novel fine-grained segmentation task by joining forces between the fashion and computer vision communities. The proposed task unifies both categorization and segmentation of rich and complete apparel attributes, an important step toward real-world applications.&rdquo;</p>

<h1 id="approach">Approach</h1>

<p><a href="https://arxiv.org/abs/1703.06870">Mask R-CNN (paper)</a> was chosen because it is currently among the best methods for this task (<a href="https://paperswithcode.com/sota/instance-segmentation-on-coco">top 10 in Instance Segmentation on COCO</a>), and it is fairly simple to be implemented.</p>

<p>Since the dataset provided is not huge (45625 images), and to save training time, it was decided to use transfer learning. The model was based on the <a href="https://github.com/matterport/Mask_RCNN">implementation by matterport</a> because they already provide a pre-trained model based on Feature Pyramid Network (FPN) and a ResNet101 backbone. The implementation is made in TensorFlow/Keras.</p>

<p>Since less than 4% of the segments have attributes, these attributes were ignored as an initial approach for simplification.</p>

<h1 id="nvidia-dgx-station">Nvidia DGX Station</h1>

<p>A Nvidia DGX Station was used for training. It consists of a system with 4 32gb Tesla V100 GPUs.</p>

<p>The communication with the station was made by SSH, so the script <a href="https://github.com/mcreduardo/fashionChallenge-SemSegmentation/blob/master/DGX_ssh_comm.py">DGX_ssh_comm.py</a> was made in order to facilitate the file transfer and automatic execution of the training during development and debugging.</p>

<p>A docker container was built by adding the requirements for the Matterport implementation to the Nvidia container  nvcr.io/nvidia/tensorflow:19.05-py3.</p>

<h1 id="training">Training</h1>

<h3 id="1-first-look-at-the-data">1) First look at the data</h3>

<ul>
<li>45625 images, 330000 sements, less than 4% of segments have attributes. Ignore attributes initially.</li>
<li>Objects have consistent vertical orientation (e.g. shoes usually have the laces on top) so it was decided to use only horizontal flip for aumentation.</li>
<li>Also shifting and scaling for aumentation.</li>
</ul>

<h3 id="2-load-and-split-dataset">2) Load and split dataset</h3>

<ul>
<li>Training Set: 80% for training, 20% for validation.</li>

<li><p>Testing Set: as it is provided by Kaggle.</p></li>

<li><p><a href="https://github.com/mcreduardo/fashionChallenge-SemSegmentation/blob/master/dataHandling.py">dataHandling.py</a></p></li>
</ul>

<h3 id="3-training-parameters">3) Training parameters</h3>

<p>The training parameters are defined in <a href="https://github.com/mcreduardo/fashionChallenge-SemSegmentation/blob/master/config.py">config.py</a></p>

<p>Some of the used parameters:</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="font-weight:bold">class</span> <span style="font-weight:bold">CustomConfigCOCO</span>(Config):
    <span style="font-style:italic">&#34;&#34;&#34;
</span><span style="font-style:italic">    My custom class using COCO pre-trained weights
</span><span style="font-style:italic">    &#34;&#34;&#34;</span>
    <span style="font-style:italic"># training</span>
    LEARNING_RATE = 2e-4
    <span style="font-style:italic"># train in three stages - 3 6 8</span>
    EPOCHS_1 = 10 <span style="font-style:italic"># head only</span>
    EPOCHS_2 = 15 + EPOCHS_1 <span style="font-style:italic"># Finetune all</span>
    <span style="font-style:italic"># instances / (GPU_COUNT * IMAGES_PER_GPU) train = 4*9125, val = 9125</span>
    STEPS_PER_EPOCH = 1520
    VALIDATION_STEPS = 380
    <span style="font-style:italic"># DGX workstation</span>
    GPU_COUNT = 4
    IMAGES_PER_GPU = 6 <span style="font-style:italic">#(32Gb/GPU)</span>
    <span style="font-style:italic"># ...</span></code></pre></div>
<p><strong>NOTE: the model was trained only once. These parameters haven&rsquo;t been tuned, they are probably not the ideal set.</strong></p>

<h3 id="4-training">4) Training</h3>

<p>The training was divided in two parts. First, only the head of the network was trained. Then, the entire network was fine-tuned. Training took 7h14min.</p>
<div class="highlight"><pre style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="font-style:italic"># Step 1 train heads</span>
model.train(train_dataset, valid_dataset,
            learning_rate=config.LEARNING_RATE,
            epochs=config.EPOCHS_1,
            layers=<span style="font-style:italic">&#39;heads&#39;</span>, <span style="font-style:italic"># train only heads, freeze rest</span>
            augmentation=augmentation)
history = model.keras_model.history.history

<span style="font-style:italic"># Step 2 fine tune all network</span>
model.train(train_dataset, valid_dataset,
            learning_rate=config.LEARNING_RATE/10,
            epochs=config.EPOCHS_2,
            layers=<span style="font-style:italic">&#39;all&#39;</span>,
            augmentation=augmentation)</code></pre></div>
<p>Loss:</p>

<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/loss.jpg" border="0"></p>
</figure>

<p>Apparently, the network overfits the training set a little bit after 17~18 epochs. Therefore, the model after 17 epochs was chosen as the final model.</p>

<h1 id="results">Results</h1>

<p>Some of the results from the test set:</p>

<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result1.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result2.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result3.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result4.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result5.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result6.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result7.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result8.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result9.jpg" border="0"></p>
</figure>
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/result10.jpg" border="0"></p>
</figure>

<h1 id="next-steps">Next steps</h1>

<ul>
<li>Tune training parameters in order to get a better model.</li>
<li>Take advantage of the fact that the position of clothing items is related to a specific body part. There is a lot of models that can detect humans and segment its body parts. A possible approach would be to use a pretrained model based on Feature Pyramid Network (FPN) and a ResNet101 backbone (such as the one used here) for feature extraction, add the detected body parts positions (detected by a different model) to the feature vector, and, finally, add the head of the model. That is:
<figure>
<p><img src="https://mcreduardo.github.io/images/fashion/schem.jpg" border="0"></p>
</figure>
As seen in the results above, many of the wrong detections would be solved. The results are expected to get better. Note that this model would increase the training and prediction time significantly, but, given the fact that the provided dataset is fairly small, this might be a good solution.</li>
<li>Test other models and architectures.</li>
<li>Ensemble various different models. Since these type of competitions don&rsquo;t require real-time prediction, that is, the computational cost for predictions is not an issue, the use of ensembles are very popular.</li>
</ul>

  </article>
</section>


      </div>

      <footer class="footer">
  <section class="container">
    
     © 2019
    
       · 
      Eduardo Rocha
     
    <span style="float:right;">
        
    </span>
  </section>
</footer>

    </main>

    

  </body>

</html>
